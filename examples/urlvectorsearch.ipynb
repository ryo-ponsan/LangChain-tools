{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eOefFOVPLraY"
      },
      "source": [
        "# URL vector search"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OjEmqBNHLral"
      },
      "source": [
        "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/fuyu-quant/url-vector-search/blob/main/examples/urlvectorsearch.ipynb)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "cQKLsiaBLrar"
      },
      "outputs": [],
      "source": [
        "%%capture\n",
        "!pip install git+https://github.com/fuyu-quant/url-vector-search.git"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "cgVvZxSlLrau"
      },
      "outputs": [],
      "source": [
        "%%capture\n",
        "!pip install langchain==0.0.153\n",
        "!pip install llama-index==0.5.27\n",
        "!pip install openai==0.27.5\n",
        "!pip install qdrant-client==1.1.6"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "k4bh694YLraw"
      },
      "outputs": [],
      "source": [
        "from urlvectorsearch import vectordatabase\n",
        "from urlvectorsearch import output\n",
        "\n",
        "import os\n",
        "\n",
        "# os.environ[\"OPENAI_API_KEY\"] = \"YOUR_API_KEY\"\n",
        "os.environ[\"OPENAI_API_KEY\"] = \"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\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "txjlhNZBLra0"
      },
      "source": [
        "### ベクトルデータベースの作成\n",
        "\n",
        "まずは検索対象となるベクトルデータベースを作成"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "iSuyP_JrLra2"
      },
      "outputs": [],
      "source": [
        "# メモリに保存する場合\n",
        "database = vectordatabase.create_vectordatabase()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "xsV17F3kLra6"
      },
      "outputs": [],
      "source": [
        "# ファイルに保存する場合\n",
        "# 好きなパスを指定してください\n",
        "path = '/content/qdrant'\n",
        "database = vectordatabase.create_vectordatabase(path = path)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3WjYZmhrLra8"
      },
      "source": [
        "### リンク先のテキストをデータベースに保存\n",
        "\n",
        "検索対象とするリンクを作成したデータベースに与えます。\n",
        "検索はURLごとの検索になります．データベースの作成には少し時間がかかりますが一度追加したものは永遠に保存されるので新しいものを追加したい時だけ実行することになります。"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "MrkBk8_zLra9"
      },
      "outputs": [],
      "source": [
        "urls = [\n",
        "    \"https://www.brainpad.co.jp/doors/knowledge/01_dx_natural_language_processing_1/\",    \n",
        "    \"https://www.brainpad.co.jp/doors/knowledge/01_quantitative_evaluation_year_2024_problem/\",   \n",
        "    \"https://www.brainpad.co.jp/doors/news_trend/logistics_industry_year_2024_problem/\",   \n",
        "]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "xvObO6CMLra_"
      },
      "outputs": [],
      "source": [
        "vectordatabase.add_text(urls, database)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oGFFjY5zLrbA"
      },
      "source": [
        "### 出力方法"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "195ozOvoLrbC"
      },
      "source": [
        "* 類似している内容のリンクと説明文とスコア付きの出力"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NrqRY9fMLrbE",
        "outputId": "dfafe712-d786-455e-d17f-cdb981569704"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/langchain/llms/openai.py:158: UserWarning: You are trying to use a chat model. This way of initializing it is no longer supported. Instead, please use: `from langchain.chat_models import ChatOpenAI`\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/langchain/llms/openai.py:661: UserWarning: You are trying to use a chat model. This way of initializing it is no longer supported. Instead, please use: `from langchain.chat_models import ChatOpenAI`\n",
            "  warnings.warn(\n"
          ]
        }
      ],
      "source": [
        "query = \"機械学習の取り組みについて\"\n",
        "# num_outputは登録したリンク先の数より小さく数にしてください\n",
        "output1 = output.description_output_with_scoer(database, query, num_output=3)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fwEjCP8WLrbG",
        "outputId": "73883d27-f760-495f-dcef-0f7667260ac6"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "この記事は、terMailが個人情報保護方針を定め、AIやデータ活用スペシャリストが最先端の情報を提供していることを紹介し、自然言語処理の歴史や技術的ブレイクスルーについて説明しています。また、Transformerを使うことで自然言語処理のタスクをコンピューターが定量的に判断できるようになり、コストの削減とビジネス上の適用範囲が拡大したことを示しています。terMailでは、様々な特集記事を配信し、最新特集記事の情報を受け取ることができます。\n",
            "https://www.brainpad.co.jp/doors/knowledge/01_dx_natural_language_processing_1/\n",
            "0.85102059056402\n",
            "この記事は、物流業界における時間外労働の上限規制による影響を検証し、数理最適化アルゴリズムを用いた配送計画シミュレーションを行った結果、効率的な配送ルートを組むことでトラック台数の削減を図る可能性があることを示しています。また、株式会社ブレインパッドアナリティクスのデータサイエンティストが、機械学習を用いた需要予測や物流業界の配送最適化などのプロジェクトに取り組んでいることも紹介されています。企業競争力を高めるためには、データ活用技術を活用した定量分析や効率的な配送計画作成が必要であることが強調されています。\n",
            "https://www.brainpad.co.jp/doors/knowledge/01_quantitative_evaluation_year_2024_problem/\n",
            "0.8461577083161491\n"
          ]
        }
      ],
      "source": [
        "print(output1['description0'])\n",
        "print(output1['url0'])\n",
        "print(output1['score0'])\n",
        "print(output1['description1'])\n",
        "print(output1['url1'])\n",
        "print(output1['score1'])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2Vat041PLrbH"
      },
      "source": [
        "* 類似している内容のリンクと説明文の出力"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "DCmeyMUTLrbJ"
      },
      "outputs": [],
      "source": [
        "query = \"最適化の取り組みについて\"\n",
        "# num_outputは登録したリンク先の数より小さく数にしてください\n",
        "output2 = output.description_output(database, query, num_output=3)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qHXV7aLFLrbK",
        "outputId": "3776d523-1f0b-429c-94b9-af8d670f1c65"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "この記事は、terMailが個人情報保護方針を定め、AIやデータ活用スペシャリストが最先端の情報を提供していることを紹介し、自然言語処理の歴史や技術的ブレイクスルーについて説明しています。また、Transformerを使うことで自然言語処理のタスクをコンピューターが定量的に判断できるようになり、コストの削減とビジネス上の適用範囲が拡大したことを示しています。terMailでは、様々な特集記事を配信し、最新特集記事の情報を受け取ることができます。\n",
            "https://www.brainpad.co.jp/doors/knowledge/01_dx_natural_language_processing_1/\n",
            "この記事は、物流業界における時間外労働の上限規制による影響を検証し、数理最適化アルゴリズムを用いた配送計画シミュレーションを行った結果、効率的な配送ルートを組むことでトラック台数の削減を図る可能性があることを示しています。また、株式会社ブレインパッドアナリティクスのデータサイエンティストが、機械学習を用いた需要予測や物流業界の配送最適化などのプロジェクトに取り組んでいることも紹介されています。企業競争力を高めるためには、データ活用技術を活用した定量分析や効率的な配送計画作成が必要であることが強調されています。\n",
            "https://www.brainpad.co.jp/doors/knowledge/01_quantitative_evaluation_year_2024_problem/\n"
          ]
        }
      ],
      "source": [
        "print(output1['description0'])\n",
        "print(output1['url0'])\n",
        "print(output1['description1'])\n",
        "print(output1['url1'])"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "output2"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XJKeoUSvWqLS",
        "outputId": "a76aeb21-4a1a-434a-b892-c46761c70153"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'description0': 'この記事は、物流業界における時間外労働の上限規制による影響を検証し、数理最適化アルゴリズムを用いた配送計画シミュレーションにより、効率的な配送ルートを組むことでトラック台数の削減を図る可能性があることを示している。また、データサイエンティストが機械学習や数理最適化を用いたプロジェクトに取り組んでいることや、物流業界における台数削減効果の実験結果などが紹介されている。企業競争力を高めるためには、データ活用技術を活用した定量分析や効率的な配送計画作成が必要であることが強調されている。',\n",
              " 'url0': 'https://www.brainpad.co.jp/doors/knowledge/01_quantitative_evaluation_year_2024_problem/',\n",
              " 'description1': 'terMailでは、最適化に関する様々な記事を配信しています。例えば、SEO最適化やコンテンツ最適化、広告最適化など、ビジネスにおいて重要な最適化手法について解説しています。また、最新の技術やトレンドについても取り上げ、ビジネスの競争力を高めるための情報を提供しています。',\n",
              " 'url1': 'https://www.brainpad.co.jp/doors/knowledge/01_dx_natural_language_processing_1/',\n",
              " 'description2': 'この記事は、2024年問題に対するトラック運送業界の最適化取り組みについて述べたものです。データガバナンスを活用し、ビジネスを加速させることができるとして、荷主への改善基準告示の啓発活動やトラック運送事業者による取り組み、理解促進が重要であると述べています。また、労働環境改善や適切な運賃料金の収受などを促進し、法令遵守の徹底事業者をコンプライアンス遵守や安全対策徹底に努めることが求められているとも述べています。',\n",
              " 'url2': 'https://www.brainpad.co.jp/doors/news_trend/logistics_industry_year_2024_problem/'}"
            ]
          },
          "metadata": {},
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SJQufwrpLrbL"
      },
      "source": [
        "* 多様性を考慮したリンクと説明文の出力"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HzBg2AWbLrbM",
        "outputId": "ca163452-3d47-4339-d24b-bca7d2afd9f3"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/langchain/llms/openai.py:158: UserWarning: You are trying to use a chat model. This way of initializing it is no longer supported. Instead, please use: `from langchain.chat_models import ChatOpenAI`\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/langchain/llms/openai.py:661: UserWarning: You are trying to use a chat model. This way of initializing it is no longer supported. Instead, please use: `from langchain.chat_models import ChatOpenAI`\n",
            "  warnings.warn(\n"
          ]
        }
      ],
      "source": [
        "query = \"機械学習の取り組みについて\"\n",
        "# num_outputは登録したリンク先の数より小さく数にしてください\n",
        "output3 = output.description_output_mmr(database, query, num_output=3, fetch_k_=10)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vwZm627TLrbM",
        "outputId": "b70c155a-8649-49f2-ae96-dfe1a03c9be7"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "この記事は、terMailが個人情報保護方針を定め、AIやデータ活用スペシャリストが最先端の情報を提供していることを紹介し、自然言語処理の歴史や技術的ブレイクスルーについて説明しています。また、Transformerを使うことで自然言語処理のタスクをコンピューターが定量的に判断できるようになり、コストの削減とビジネス上の適用範囲が拡大したことを示しています。terMailでは、様々な特集記事を配信し、最新特集記事の情報を受け取ることができます。\n",
            "https://www.brainpad.co.jp/doors/knowledge/01_dx_natural_language_processing_1/\n",
            "この記事は、物流業界における時間外労働の上限規制による影響を検証し、数理最適化アルゴリズムを用いた配送計画シミュレーションを行った結果、効率的な配送ルートを組むことでトラック台数の削減を図る可能性があることを示しています。また、株式会社ブレインパッドアナリティクスのデータサイエンティストが、機械学習を用いた需要予測や物流業界の配送最適化などのプロジェクトに取り組んでいることも紹介されています。企業競争力を高めるためには、データ活用技術を活用した定量分析や効率的な配送計画作成が必要であることが強調されています。\n",
            "https://www.brainpad.co.jp/doors/knowledge/01_quantitative_evaluation_year_2024_problem/\n"
          ]
        }
      ],
      "source": [
        "print(output1['description0'])\n",
        "print(output1['url0'])\n",
        "print(output1['description1'])\n",
        "print(output1['url1'])"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "---\n",
        "\n",
        "# 【第二章】下記ライブラリを使用した場合\n",
        "- LangChain\n",
        "  - LLM との連携をする上でとても使いやすいので使用しています。今後 OpenAI の LLM 以外の LLM を使う際も簡単に切り替えられるようになっていくと思っているのでその点でも採用しています。\n",
        "- Qdrant\n",
        "  - ベクトル検索とベクトルデータベースが使いやすいのと LangChain からも使えるので採用しました。\n",
        "- llama-index\n",
        "  - LangChain の URL tool がうまく実行できなかったため URL からの情報の取得には llama-index を使いました．\n",
        "- OpenAI\n",
        "#  - OpenAI の LLM を利用するため使っています。"
      ],
      "metadata": {
        "id": "CgDU9yzBZdLR"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 1.URLからデータ取得\n",
        "\n",
        "llama_indexの機能を使いURLを渡し，テキスト情報の抽出"
      ],
      "metadata": {
        "id": "AxBBcV-XZpek"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from llama_index.readers import BeautifulSoupWebReader\n",
        "\n",
        "urls = [\n",
        "    \"https://www.brainpad.co.jp/doors/knowledge/01_dx_natural_language_processing_1/\",    \n",
        "    \"https://www.brainpad.co.jp/doors/knowledge/01_quantitative_evaluation_year_2024_problem/\"\n",
        "]\n",
        "\n",
        "documents = BeautifulSoupWebReader().load_data(urls=urls)\n"
      ],
      "metadata": {
        "id": "zkBVrdtIYj-4"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 1つ目のリンクに関する情報\n",
        "documents[0].text"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 140
        },
        "id": "2S9C7hciZxeQ",
        "outputId": "f4251a3c-3e14-4cbe-bf70-87a4f86fc1f0"
      },
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n \\n\\n\\n\\n\\n【前編】自然言語処理技術を用いた「テキストデータ」のビジネス活用の現在地 | DX・データ活用情報発信メディア-DOORS DX\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n \\n\\n\\n\\n\\n\\nベストなDXへの入り口が見つかるメディア\\n\\n\\n\\n\\n\\n\\nナレッジ\\nニュース・トレンド\\n特集\\nスペシャリスト一覧\\n資料\\nメルマガ\\nDOORSとは\\n\\n\\n\\nタグ一覧\\n検索\\n\\n\\n\\n検索\\n\\n \\n\\n\\n\\nMENU\\n\\n\\n\\n\\n\\nトップ\\nナレッジ\\n【前編】自然言語処理技術を用いた「テキストデータ」のビジネス活用の現在地\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n【前編】自然言語処理技術を用いた「テキストデータ」のビジネス活用の現在地\\n\\n\\n2022.01.06\\n\\n[執筆者]\\n梅田 義章\\n\\n\\n\\n\\n#自然言語処理\\n#AI／機械学習\\n\\n\\n\\n\\n\\n \\nINDEX \\n\\n \\n1\\nビジネスで使える自然言語処理、使えない自然言語処理とは？\\n\\n \\n2\\n2013年に起きた技術的ブレイクスルー\\n\\n \\n3\\n「Transformer」で一気にビジネスに使えるようになった理由とは？\\n\\n\\nビジネスで使える自然言語処理、使えない自然言語処理とは？\\nAIプラクティス部で各プロジェクトの成果物レビューを担当するかたわら、自然言語処理を研究している梅田義章です。自然言語処理への関わりは2005年からですので、今年で17年目になりました。その間に大きなブレイクスルーが何回かあり、現在に至っています。\\n他のAI応用と比較すると「自然言語処理のビジネス利用」は遅れていました。ビジネスの現場で活用の頻度が高まってきたのは最近のことです。理由は単純で、「ビジネスで使える段階でなかった」だけです。\\n「ビジネスで使える／使えない」とはどういうことでしょうか。\\nルールベースであれ、機械学習であれ、AIの構築には手間も時間もお金（長いので、これら3つをあわせて「コスト」と呼びます）もかかります。10年くらい前までは、このコストが大変かかりました。複数のAIを組み合わせて目的を達成する必要があったからです。AIの各ステップには大きなコストがかかったにもかかわらず、コストを上回る価値を生み出すビジネス的な応用が少なかったのです。これが「ビジネスで使えない」の意味です。しかし後述する技術的ブレイクスルーの恩恵で、「AIを作成するコスト」が「AIが生み出す価値」を下回るようになりました。\\nそしてブレイクスルーのたびにコストが小さくなりながら、AIにできることも増えていきました。どんどん「ビジネスで使える」ようになってきたわけです。それでここ数年になって、日本でも先進的な企業が自然言語処理をビジネスに活用しはじめたということなのです。\\n2013年に起きた技術的ブレイクスルー\\nプログラミング言語やエスペラントなどを人工言語と言います。これらは極めてロジカルにできています。プログラミング言語がロジカルでなければコンピューターでは処理できませんし、エスペラントに例外的な複数形や不規則動詞などがあったら習得が難しくなり普及しません。\\n人工言語に対して自然言語はあまりロジカルとは言えません。今挙げた複数形や不規則動詞などのように文法は例外だらけですし、話し言葉になると「正しい文法」が守られているとは限りません。守られないので「正しい文法」が時代とともに変わっていきます。新しい言葉もたくさん出てきます。日本語に至っては、単語に分けるのが難しいという側面もあります。\\nそれでも2005年の段階で、構文解析、意味解析、文脈解析などの研究はかなり進んでいました。ただ私が思うには、ビジネスで使える技術は形態素解析ぐらいでした。形態素とは、意味を持つ表現要素の最小単位を指す言語学の用語です。\\n日本語の形態素解析は、コンピューター上で日本語のかなから、かな漢字混じりの文章を構築する研究に伴って発展した分野です。検索エンジンには欠かせない技術ですし、日本語と関連したレコメンドエンジンでも使われています。スパムメールを見分ける際にも、特定の形態素がどれくらい含まれているかが判断基準になっています。ビジネス応用といっても、インフラ的な技術ですが、しかし日本語入力や検索はもはやビジネスに欠かせませんし、レコメンドなどは直接利益を生み出します。\\n大量データを決まった方法ですばやく処理するというコンピューターの利点を生かすことができたため、また得られる結果の精度も高かったため、形態素解析の活用は成功しました。しかし、他の分野は研究が進んでいたのにビジネスでは使えませんでした。コストが掛かる割に精度が低く、ビジネス応用ができなかったからです。\\nAIであろうと他のソフトウェアと同じくコンピューター上で動作します。コンピューターは元々数値を扱うことは得意ですが、文字列をそのまま扱うことは得意ではありません。\\nでは言語を数値に変換すればいいのでは――そのような考え方を実現したブレイクスルーが2013年に起こりました。「単語の分散表現」と呼ばれる技術で、1つの単語を数10〜数100程度の次元のベクトルで表現することを言います。ベクトルとは数字の羅列で、（1, 0.2, 1.3）のように表現します。並べられた数字の個数を次元と言い、この例では3次元となります。\\n要するに単語を数字の羅列として表現するようになっただけなのですが、これがコンピューターにとっては大きなブレイクスルーだったのです。たとえば似たような単語かどうかは、ベクトルの中の数字がどれだけ似ているかで定量的に判断できます。それまでは、人間が目で見て、単語同士が似ているかどうかを調べていました。しかし人によって似ている度合いが違う定性的な評価であり、精度が低かったのです。定量的に判断できるということは誰がやっても同じということであり、もちろんコンピューターにもできるということです。\\nしかもその計算が、高校で習うレベルの数学、つまりコンピューターにとっては簡単な計算なので処理時間（＝コスト）もほとんどかかりません。\\n「Transformer」で一気にビジネスに使えるようになった理由とは？\\n続くブレイクスルーは2016年に起こりました。\\nご存知の方も多いと思いますが、深層学習（ディープラーニング）は、多層ニューラルネットワークという数理モデルをベースとしています。その一種にRNN（再帰型ニューラルネットワーク）というものがあります。これはニューラルネットワークを時系列データが扱えるように拡張したものです。\\n自然言語も基本的に前から後に流れる時系列データと捉えることができるため、Transformer以前の言語モデリングではRNNがよく使われていました。細かい説明は省きますが、2016年に「Attention付きSeq2Seqモデル」が登場し、RNNベースでの深層学習の精度が飛躍的に高まりました。\\n仕事や勉強でWebを使う人は、この頃からGoogle翻訳の精度が急に向上したことを憶えているのではないでしょうか。これはまさにAttention付きSeq2Seqモデルの恩恵です。\\nRNNベースでの深層学習や形態素の分散表現により、各分析タスクは前処理を含めて一気通貫で学習できるようになり、コストの大幅な削減につながりました。また、人間が時間を掛けて構築した以前のモデルより、深層学習任せのモデルの精度が高くなり、ビジネス上の適用範囲が増えました。\\nさらに大きなブレイクスルーが翌年の2017年に訪れます。「Transformer」です。\\nRNNには欠点があって、それは逐次処理しかできないということです。モデルの精度を高めるには、大量の学習データを使用することと学習時に与えるパラメータ数を増やすことが一般的です。逐次処理しかできなければ、精度を高めるために膨大な時間がかかることになります。\\n一方でソフトウェアの処理速度を飛躍的に高める方法としてよく採用されるのは、分散環境で同時に処理を行う並列計算です。Transformerは、Attention付きSeq2SeqモデルのAttentionの部分を利用したモデルです。逐次処理を必要とせず、並列処理が可能となっています。\\nTransformerモデルでは、RNNモデルと比較して計算量は膨大になりましたが、その分精度が大幅に向上し、ビジネス上の適用範囲が増えました。また、各種タスクに用いるモデルの基本部分を共通化できるため、複数用途に利用する際のコストが削減されています。\\nなおTransformerは、モデル構築のベースとなるモデルです。Transformerの仕組みを利用し、学習方法や一部の内部ロジックを改善したモデルが多数発表されています。最も有名なのがGoogleのBERTで、2018年に登場しました。その他にもGPT、XLNet、ALBERT、ELECTRAなどがTransformerモデルとしてよく知られています。\\n\\n\\nこの記事の続きはこちら\\n\\n\\n\\n\\nDOORS-BrainPad DX Media-\\n【後編】自然言語処理技術を用いた「テキストデータ」のビジネス活用の現在地\\n\\n\\n\\n\\n\\n前の記事へ\\n次の記事へ\\n\\n\\n\\n\\n\\n\\nこの記事を読んだ方におすすめ\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nRANKING人気ランキング\\n\\n\\n\\n\\n1DX（デジタルトランスフォーメーション）とは？IT化との違いや事例を解説\\n\\n\\n\\n\\n\\n2運送業界の「2024年問題」とは？業界の現状から考える解決法\\n\\n\\n\\n\\n\\n3ChatGPTとは？使い方・始め方・仕組み・最新の活用事例を一挙ご紹介！\\n\\n\\n\\n\\n\\n4マテリアルズ・インフォマティクスとは？材料開発・研究を加速させるデータの力\\n\\n\\n\\n\\n\\n5強化学習とは？これから学びたい人のための基礎知識や活用事例を紹介\\n\\n\\n\\n\\n\\n6社内文書に特化したChatGPT\\u3000ファインチューニング実践編\\n\\n\\n\\n\\n\\nNEW ARTICLES新着記事\\n\\n\\n\\n\\nバロックジャパンと考えるパーソナライズとUGC活用\\n\\n\\n\\n\\n\\nリスキリングとは？DX時代の人材育成に必要な考え方と方法論\\n\\n\\n\\n\\n\\nDX（デジタルトランスフォーメーション）とは？IT化との違いや事例を解説\\n\\n\\n\\n\\n\\n社内文書に特化したChatGPT\\u3000ファインチューニング実践編\\n\\n\\n\\n\\n\\nCDPを活用したスピーディーなデータ収集、加工\\n\\n\\n\\n\\n\\n【シリーズ】データガバナンスがもたらすもの－第9回\\u3000データドリブン文化醸成とは（後編）\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n \\n\\n\\nRANKING人気ランキング\\n\\n\\n1DX（デジタルトランスフォーメーション）とは？IT化との違いや事例を解説\\n2運送業界の「2024年問題」とは？業界の現状から考える解決法\\n3ChatGPTとは？使い方・始め方・仕組み・最新の活用事例を一挙ご紹介！\\n4マテリアルズ・インフォマティクスとは？材料開発・研究を加速させるデータの力\\n5強化学習とは？これから学びたい人のための基礎知識や活用事例を紹介\\n6社内文書に特化したChatGPT\\u3000ファインチューニング実践編\\n\\n\\n\\n\\nNEW ARTICLES新着記事\\n\\n\\nバロックジャパンと考えるパーソナライズとUGC活用\\nリスキリングとは？DX時代の人材育成に必要な考え方と方法論\\nDX（デジタルトランスフォーメーション）とは？IT化との違いや事例を解説\\n社内文書に特化したChatGPT\\u3000ファインチューニング実践編\\nCDPを活用したスピーディーなデータ収集、加工\\n【シリーズ】データガバナンスがもたらすもの－第9回\\u3000データドリブン文化醸成とは（後編）\\n\\n\\n\\n\\n\\n\\nWRITER執筆者プロフィール\\n\\n\\n\\n\\n株式会社ブレインパッド \\nアナリティクス本部AIプラクティス部 梅田 義章\\n北海道大学理学研究科修了、素粒子理論を専攻。ハンブルグ大学、アーヘン工科大学、台湾交通大学でのポスドク、前職データ分析会社を経て2013年にブレインパッド入社。社内ではレビュアーとして幅広い分析案件に対する提案、進行、納品の品質管理や技術支援を担当。また自然言語処理についての研究・開発を担当。\\n\\n\\n\\n\\n\\n\\nこの記事をシェア\\n\\n\\nFacebook\\nTwitter\\nMail\\n\\n\\n\\n\\n\\nMAIL MAGAZINEメールマガジン\\nデータ活用の厳選記事や、会員限定のDXのお得情報などをお届けいたします。\\n\\nメールマガジンのご案内\\n\\n \\n\\n\\n\\n\\n\\n\\nTREND人気ワード・タグ\\n\\n\\n# RPA\\n# SCM\\n# DevSecOps\\n# メディア・広告・その他\\n# AI／機械学習\\n# 電力・エネルギー・建設\\n# SaaS\\n# IT・情報・通信\\n# 基盤構築\\n# DOORS Conference\\n# 旅行・航空・運輸\\n# 金融\\n# 画像解析\\n# 製造\\n# コンサルティング\\n# デジタルマーケティング\\n# 数理最適化\\n# 小売・流通・卸売\\n# 人材育成／リスキリング\\n# 農業\\n\\n\\n人気ワード・タグ一覧\\n\\n\\n\\n\\n\\nFacebook\\nTwitter\\nMail\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n提供会社\\nセキュリティポリシー\\n個人情報保護方針\\n個人情報の取扱い\\n著作権\\n\\n\\n©️2020 BrainPad Inc.\\n\\n\\n\\nPAGE TOP\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n閉じる\\nBEST PRACTICEベストプラクティス\\n\\n\\n\\n業態・業種から探す\\n\\n\\n# 農業\\n# 小売・流通・卸売\\n# 製造\\n# 金融\\n# 旅行・航空・運輸\\n# IT・情報・通信\\n# 電力・エネルギー・建設\\n# メディア・広告・その他\\n\\n\\n\\n\\nテーマから探す\\n\\n\\n# 需要予測\\n# DXの基礎知識\\n# 内製化\\n# レコメンド\\n# PoC\\n# 画像認識\\n# MA\\n# データガバナンス\\n# MLOps\\n# ESG\\n# セールスイネーブルメント\\n# マテリアルズ・インフォマティクス\\n# 自然言語処理\\n# ダイナミック・プライシング\\n# 事例\\n# 人材育成／リスキリング\\n# データエンジニアリング\\n# 数理最適化\\n# コンサルティング\\n# デジタルマーケティング\\n# 画像解析\\n# DOORS Conference\\n# 基盤構築\\n# SaaS\\n# AI／機械学習\\n# DevSecOps\\n# SCM\\n# RPA\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nデータ活用のプロが考える、エンジニアリング視点の記事や、新規ビジネスの創出に関する記事まで幅広い特集を配信！\\n\\n特集記事一覧へ\\n\\n\\n\\n\\n\\n\\n業界の最先端をいく、100名を超える当社に在籍のAIおよびデータ活用スペシャリストが原稿を執筆！\\n\\n執筆者一覧へ\\n\\n\\n\\n\\nCLOSE\\n\\n \\n\\nベストなDXへの入り口が見つかるメディア\\n\\n\\n\\nナレッジ\\nニュース・トレンド\\n特集\\nスペシャリスト一覧\\n資料\\nメルマガ\\nDOORSとは\\n\\n\\n\\nタグ一覧\\n検索\\n\\n\\n\\n検索\\n\\n \\n\\n\\n提供会社\\nセキュリティポリシー\\n個人情報保護方針\\n個人情報の取扱い\\n著作権\\n\\n\\n\\n\\n \\n\\n\\n\\n閉じる\\nMAIL MAGAZINEメールマガジン\\n\\n\\n\\n登録が完了しました。\\nメールマガジンのご登録ありがとうございます。最新特集記事の情報をお届けしますので、お楽しみにお待ちください。\\n\\n\\n\\n\\n\\n\\n\\n\\n閉じる\\nMAIL MAGAZINEメールマガジン\\n\\n\\n\\n登録エラーです。\\n\\n\\n\\n\\n \\n\\n\\n\\n\\n\\n\\n\\n\\n'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 27
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 2.テキストデータの要約\n",
        "- 検索クエリに対して内容を反映した出力を生成する際にテキストデータが長くなるとLLMに入力する事ができなくなる\n",
        "- 検索後にヒットしたものを要約するようにすると最終的な出力を生成するのに時間がかかってしまう\n",
        "\n",
        "ここの項目は，「テキストデータが長くない場合」や「時間はかけて良いが正確な出力が欲しい場合」は省力してよい  "
      ],
      "metadata": {
        "id": "qobANA3Gczcj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.text_splitter import CharacterTextSplitter\n",
        "from langchain.docstore.document import Document\n",
        "\n",
        "# text =  \"機械学習プロジェクトを推進するにあたって大切なこと～DX推進時....\"\n",
        "# text =  documents[0].text\n",
        "\n",
        "# 複数の URL を読み込んでいる場合、それぞれの URL から得られる Document の text を連結\n",
        "text = \"\".join([doc.text for doc in documents])\n",
        "\n",
        "# 1000文字ごとに'\\n\\n'を挿入する\n",
        "chunks = [text[i:i+1000] for i in range(0, len(text), 1000)]\n",
        "result = '\\n\\n'.join(chunks)\n",
        "\n",
        "# '\\n\\n'で文字列を分割し一つのチャンクとする\n",
        "text_splitter = CharacterTextSplitter(\n",
        "    separator = \"\\n\\n\",  # 文章を分割する文字列\n",
        "    chunk_size = 1000,  # チャンクの文字数\n",
        "    chunk_overlap = 0,  # チャンク間で重複させる文字数\n",
        ")\n",
        "\n",
        "# 分割した文字列をLLMに入力するデータ形式に変換する\n",
        "split_texts = text_splitter.split_text(result)\n",
        "split_docs = [Document(page_content=t) for t in split_texts]\n"
      ],
      "metadata": {
        "id": "BuXi1SQaZ4_4"
      },
      "execution_count": 28,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "またここでは[map_reduce](https://note.com/mega_gorilla/n/n6f46fc1985ca)という方法を使用し、二段階に分けて要約を行うのでそれぞれの段階でのプロンプトを作成"
      ],
      "metadata": {
        "id": "2tJFED6Xj2Mj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain import PromptTemplate\n",
        "\n",
        "template1 = \"\"\"\n",
        "次の文章を日本語で500文字程度に要約してください．\n",
        "文章：{text}\n",
        "\"\"\"\n",
        "\n",
        "template2 = \"\"\"\n",
        "次の文章を日本語で1200文字程度に要約してください．\n",
        "文章：{text}\n",
        "\"\"\"\n",
        "\n",
        "prompt1 = PromptTemplate(\n",
        "    input_variables = ['text'],\n",
        "     template = template1,\n",
        ")\n",
        "\n",
        "prompt2 = PromptTemplate(\n",
        "    input_variables = ['text'],\n",
        "     template = template2,\n",
        ")\n"
      ],
      "metadata": {
        "id": "-gM8bAIWja1p"
      },
      "execution_count": 32,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 3.要約の実行"
      ],
      "metadata": {
        "id": "wId5X7YykCi5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.chains.summarize import load_summarize_chain\n",
        "from langchain.llms import OpenAI\n",
        "\n",
        "llm = OpenAI(temperature=0, max_tokens = 1200)\n",
        "\n",
        "chain = load_summarize_chain(\n",
        "    llm = llm, \n",
        "    chain_type=\"map_reduce\",\n",
        "    # それぞれの要約を行うときのテンプレ\n",
        "    map_prompt = prompt1,\n",
        "    # 要約文の要約文を作るときのテンプレ\n",
        "    combine_prompt = prompt2\n",
        "    )\n",
        "\n",
        "summary = chain.run(split_docs)\n",
        "\n",
        "summary"
      ],
      "metadata": {
        "id": "im5mkoYvjmnt",
        "outputId": "89c5c8c4-3b54-4452-d1aa-f1dd0289e67d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 157
        }
      },
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:langchain.llms.openai:Retrying langchain.llms.openai.completion_with_retry.<locals>._completion_with_retry in 4.0 seconds as it raised RateLimitError: The server had an error while processing your request. Sorry about that!.\n",
            "WARNING:langchain.llms.openai:Retrying langchain.llms.openai.completion_with_retry.<locals>._completion_with_retry in 4.0 seconds as it raised RateLimitError: The server had an error while processing your request. Sorry about that!.\n",
            "WARNING:langchain.llms.openai:Retrying langchain.llms.openai.completion_with_retry.<locals>._completion_with_retry in 4.0 seconds as it raised RateLimitError: The server had an error while processing your request. Sorry about that!.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'\\n2013年以降、Word2VecやDeep Learning、Transformerなどの技術的ブレイクスルーにより、自然言語処理のコストが大幅に減少し、ビジネスで使える段階にまで押し上げられました。BrainPadでは、これらの技術を活用した新しいサービスや技術の開発を行っています。ナレッジメディア「DOORS」では、AIやデータ活用技術を活用した新規ビジネスの創出を支援するための情報を提供しています。今後も自然言語処理技術は発展していくと考えられますので、エンジニアとして活躍してみませんか？2013年以降、Word2VecやDeep Learning、Transformerなどの技術的ブレイクスルーにより、自然言語処理のコストが大幅に減少し、ビジネスで使える段階にまで押し上げられました。BrainPadでは、これらの技術を活用した新しいサービスや技術の開発を行っています。ナレッジメディア「DOORS」では、AIやデータ活用技術を活用した新規ビジネスの創出を支援するための情報を提供しています。今後も自然言語処理技術は発展し続け、ビジネスで活用される可能性が高いです。BrainPadでは、これらの技術を活用した新しいサービスや技術の開発を行っています。ナレッジメディア「DOORS」では、AIやデータ活用技術を活用した新規ビジネスの創出を支援するための情報を提供しています。今後も自然言語処理技術は発展し続け、ビジネスで活用される可能性が高いです。そのため、エンジニアとして活躍してみるのも良いでしょう。'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 34
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "---\n",
        "\n",
        "## 4.検索データベースの作成\n",
        "\n",
        "Qdrantを使った検索対象の文章を保存するためのベクトルデータベース作成\n",
        "Qdrantの特徴\n",
        "- 外部APIを使わなくてもベクトルデータベースを作成できる\n",
        "- [さまざまな保存場所を選ぶ事ができる](https://qiita.com/fuyu_quant/items/4a2fe7fad49f8702e2ec#qdrant)\n",
        "- [LangChainで実装できる](https://python.langchain.com/en/latest/modules/indexes/vectorstores/examples/qdrant.html)\n"
      ],
      "metadata": {
        "id": "K4H_KoMale2P"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Qdrantのベクトルデータベースの作成\n",
        "\n",
        "from langchain.embeddings.openai import OpenAIEmbeddings\n",
        "from langchain.vectorstores import Qdrant\n",
        "from langchain.schema import Document\n",
        "\n",
        "embeddings = OpenAIEmbeddings()\n",
        "\n",
        "from llama_index.readers import BeautifulSoupWebReader\n",
        "\n",
        "urls = [\n",
        "    \"https://tech-deliberate-jiro.com/pointcloud-deeplearning/#\",\n",
        "    \"https://qiita.com/KYoshiyama/items/802506ec397559725a1c\"\n",
        "]\n",
        "\n",
        "# documents = BeautifulSoupWebReader().load_data(urls=urls)\n",
        "documents = BeautifulSoupWebReader().load_data(urls=urls)\n",
        "text = documents[0].text\n",
        "\n",
        "# サンプルのデータ\n",
        "# sample = [Document(page_content=\"sample\", metadata={\"url\": \"https://tech-deliberate-jiro.com/pointcloud-deeplearning/#\" })]\n",
        "sample = [Document(page_content=text, metadata={\"url\": \"https://tech-deliberate-jiro.com/pointcloud-deeplearning/#\" })]\n",
        "# ベクターインデックスを作成する\n",
        "qdrant = Qdrant.from_documents(sample, embeddings, location=\":memory:\", collection_name= \"my_documents\")"
      ],
      "metadata": {
        "id": "BKsXT5HSkJVL"
      },
      "execution_count": 36,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 2つ目以降のデータ登録"
      ],
      "metadata": {
        "id": "-biAB4dX3K__"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "text2 = documents[1].text\n",
        "\n",
        "# textをLangChainで扱えるようにデータ形式を変換する\n",
        "# metadataの登録はしてもしなくても良い\n",
        "docs = [Document(page_content=text2, metadata={\"url\": \"https://www.brainpad.co.jp/doors/knowledge/01_data_governance_9_1/\" })]\n",
        "\n",
        "qdrant.add_documents(docs, collection_name=\"my_documents\")"
      ],
      "metadata": {
        "id": "NjMM4_Z13Odn",
        "outputId": "d9c76176-ce72-49e4-a7f7-c193a52754cf",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 38,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['788011efb8a444a7a6b8ca777352fd65']"
            ]
          },
          "metadata": {},
          "execution_count": 38
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 検索の実行\n",
        "\n",
        "上記の方法で作成したベクトルデータベースに対して検索を行います．検索方法にはスコア付きの出力を得るものや[MMR](https://yolo-kiyoshi.com/2020/05/08/post-1781/#outline__2)という類似度の高さ以外に取得するデータの多様性を高める方法などがある"
      ],
      "metadata": {
        "id": "eC43hzU25lBs"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "query = \"点群深層学習とは\"\n",
        "\n",
        "# 類似度スコア付きの出力\n",
        "# 類似度スコアが高いものから順番に取得してきます\n",
        "found_docs = qdrant.similarity_search_with_score(query)\n",
        "document, score = found_docs[0]\n",
        "print(document.page_content)\n",
        "print(f\"\\nScore: {score}\")"
      ],
      "metadata": {
        "id": "z2rALG4I3OIY",
        "outputId": "97ee7130-7796-49fe-a2d6-4c2d37a50ba8",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 41,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "点群×ディープラーニング【入門】 - Qiita\n",
            "searchsearchLoginSignupTrendQuestionOfficial EventOfficial ColumnOpportunitiesOrganization\n",
            "\n",
            "\n",
            "【6/16(金)@渋谷】エンジニア職限定採用イベント「【25卒向け】Qiita Career Meetup for STUDENT」🚀close155160more_horizinfoMore than 3 years have passed since last update.@KYoshiyama(吉山 恭平)posted at 2019-06-05updated at 2019-06-05点群×ディープラーニング【入門】sellPython,MachineLearning,AI,CNN,PointCloud\n",
            "\n",
            "はじめに\n",
            "現在主流となっている点群処理モデル「PointNet」について解説する。\n",
            "（なお、PointNetは2017年に提案されたモデルであり、その後さらに精度の高いモデルが多数提案されている。気になる方はこちらを参照。）\n",
            "また、特に断りのない限り，本記事で使用する画像は、こちらの論文1からの引用。\n",
            "\n",
            "従来手法: ３次元畳み込みニューラルネットワーク\n",
            "Volumetric CNN：点群をボクセル化し、一層一層を画像として扱うことで、畳み込みニューラルネットワークの入力とする。\n",
            "各ボクセルは 2 値で表現し，メッシュからボクセルに変換する際にボクセル内にデータがあるかどうかで 0 か 1 の値を決めて表現する。\n",
            "Volumetric CNNの欠点として、ボクセル化する際に、点群データの空間特性が必然的に変化し、物体の凹凸表現が欠損してしまうことが挙げられる。\n",
            "\n",
            "参照：VoxNet: A 3D Convolutional Neural Network for Real-Time Object Recognition2\n",
            "\n",
            "PointNet\n",
            "\n",
            "PointNetは、クラス分類、セグメンテーション、検出など応用範囲が広く、与えられた点群全体に対してクラス分類を行うこともできるし、点群をセグメントに分割し、それぞれのセグメントに対してラベル付けすることもできる。\n",
            "先述したVolumetric CNNで起きる問題に対し、PointNetは点群の空間特性を最大限残すための独自の点群処理方式を導入した。これにより、従来手法を超えるパフォーマンスを叩き出した。\n",
            "PointNetで提案された点群処理方法の特徴は次の２点である：\n",
            "\n",
            "特徴その１： 頂点インデックスの順番による影響をなくす\n",
            "特徴その２： 回転による影響をなくす\n",
            "\n",
            "本記事では、上記の課題について、PointNetがどのようなアプローチを取ったかを解説する。\n",
            "\n",
            "PointNetの構造\n",
            "まずは、ネットワーク全体の構成について。\n",
            "詳しい説明は省略するが、上段でクラス分類を行い、下段ではsemantic segmentationやpart segmentationを行う。\n",
            "\n",
            "\n",
            "特徴その1. 頂点インデックスの順番による影響をなくす\n",
            "\n",
            "参照：Point Cloud Data Using Deep Learning3\n",
            "上図のように、点群データは頂点インデックスを変えても本質的には何も変わらないため、データの順番に左右されないようなモデルを構築する必要がある。\n",
            "そのための方法として、以下の３つが挙げられる。\n",
            "\n",
            "\n",
            "一定の規則に従って、頂点をソートし直す ⇛  高次元空間での並べ替えは必ずしも上手くいかないのでダメ \n",
            "\n",
            "Data augmentation(データ拡張)によって、頂点の順番に影響を受けにくいモデルを作る ⇛  データが爆発的に増えて現実的ではない（頂点がN個あったら、組み合わせはN!通り）\n",
            "\n",
            "対称関数を使ったモデルを組む ⇛  対称関数(symmetric function)とは...？\n",
            "\n",
            "\n",
            "対称関数とは\n",
            "対称関数とは、変数の順番を入れ替えても値が変わらない関数のこと。\n",
            "例として、以下のような関数がある。\n",
            "\\begin{aligned}\n",
            "f(x_1, x_2, ..., x_n) &= x_1 + x_2 + \\cdots + x_n \\\\\n",
            "f(x_1, x_2, ..., x_n) &= \\max\\{x_1, x_2, ..., x_n\\}\n",
            "\\end{aligned}\n",
            "\n",
            "例えば、$x_1$と$x_2$を入れ替えても、値は変わらないので、対称関数と言える。\n",
            "PointNetではこの対称関数の考え方を応用しており、全ての頂点に対して同じ重みの多層パーセプトロンをかけたり、プーリング層ではMax poolingを行ったりすることで全ての頂点を平等に扱うことに成功している。\n",
            "\n",
            "特徴その2. 回転による影響をなくす\n",
            "\"T-Net\"と呼ばれるネットワークを組み込むことで、アフィン変換行列を推定し、その変換行列をかけることで回転による影響をなくすようにしている。\n",
            "\n",
            "\n",
            "まとめ\n",
            "最近の点群処理でよく使われる深層学習モデル PointNet について、解説した。\n",
            "対称関数やアフィン変換行列を用いた点群処理モデル特有の工夫を説明した。\n",
            "PointNetは、空間の細かい情報が抜けやすかったり、点密度を均一にすると精度が落ちたりといった欠点があったが、それを解決する手法 PointNet++ も提案されているので、興味があればぜひ論文を読んでいただきたい。\n",
            "\n",
            "おわりに\n",
            "ヒトやモノをデータ化＆解析してみたい、という方。\n",
            "３D技術と深層学習を組み合わせて、何か面白いサービスを作ってみたい！、という方。\n",
            "弊社では一緒に働いてくれる仲間を大募集しています。\n",
            "ご興味がある方は下記リンクから是非ご応募ください！\n",
            "https://about.sapeet.com/recruit/\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "PointNet: Deep Learning on Point Sets for 3D Classification and Segmentation ↩\n",
            "\n",
            "\n",
            "VoxNet: A 3D Convolutional Neural Network for Real-Time Object Recognition ↩\n",
            "\n",
            "\n",
            "Point Cloud Data Using Deep Learning ↩\n",
            "\n",
            "\n",
            "\n",
            "Register as a new user and use Qiita more convenientlyYou get articles that match your needsYou can efficiently read back useful informationWhat you can do with signing upSign upLogin155160more_horiz\n",
            "\n",
            "How developers code is here.© 2011-2023Qiita Inc.Guide & HelpAboutTermsPrivacyGuidelineDesign GuidelineFeedbackHelpAdvertisementContentsRelease NoteOfficial EventOfficial ColumnOpportunitiesAdvent CalendarQiita AwardAPISNS@Qiita@qiita_milestone@qiitapoi@QiitaOur serviceQiita TeamQiita JobsQiita ZineOfficial ShopCompanyAbout UsCareersQiita Blog\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Score: 0.864924562983804\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# スコアなしの出力\n",
        "# 類似度が高いものから順番に取得してきます\n",
        "found_docs = qdrant.similarity_search(query)\n",
        "print(found_docs[0].page_content)"
      ],
      "metadata": {
        "id": "WDOlDhkR6xke",
        "outputId": "95fa5fda-efbd-405b-9bfa-1fa935a8d210",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 43,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "点群×ディープラーニング【入門】 - Qiita\n",
            "searchsearchLoginSignupTrendQuestionOfficial EventOfficial ColumnOpportunitiesOrganization\n",
            "\n",
            "\n",
            "【6/16(金)@渋谷】エンジニア職限定採用イベント「【25卒向け】Qiita Career Meetup for STUDENT」🚀close155160more_horizinfoMore than 3 years have passed since last update.@KYoshiyama(吉山 恭平)posted at 2019-06-05updated at 2019-06-05点群×ディープラーニング【入門】sellPython,MachineLearning,AI,CNN,PointCloud\n",
            "\n",
            "はじめに\n",
            "現在主流となっている点群処理モデル「PointNet」について解説する。\n",
            "（なお、PointNetは2017年に提案されたモデルであり、その後さらに精度の高いモデルが多数提案されている。気になる方はこちらを参照。）\n",
            "また、特に断りのない限り，本記事で使用する画像は、こちらの論文1からの引用。\n",
            "\n",
            "従来手法: ３次元畳み込みニューラルネットワーク\n",
            "Volumetric CNN：点群をボクセル化し、一層一層を画像として扱うことで、畳み込みニューラルネットワークの入力とする。\n",
            "各ボクセルは 2 値で表現し，メッシュからボクセルに変換する際にボクセル内にデータがあるかどうかで 0 か 1 の値を決めて表現する。\n",
            "Volumetric CNNの欠点として、ボクセル化する際に、点群データの空間特性が必然的に変化し、物体の凹凸表現が欠損してしまうことが挙げられる。\n",
            "\n",
            "参照：VoxNet: A 3D Convolutional Neural Network for Real-Time Object Recognition2\n",
            "\n",
            "PointNet\n",
            "\n",
            "PointNetは、クラス分類、セグメンテーション、検出など応用範囲が広く、与えられた点群全体に対してクラス分類を行うこともできるし、点群をセグメントに分割し、それぞれのセグメントに対してラベル付けすることもできる。\n",
            "先述したVolumetric CNNで起きる問題に対し、PointNetは点群の空間特性を最大限残すための独自の点群処理方式を導入した。これにより、従来手法を超えるパフォーマンスを叩き出した。\n",
            "PointNetで提案された点群処理方法の特徴は次の２点である：\n",
            "\n",
            "特徴その１： 頂点インデックスの順番による影響をなくす\n",
            "特徴その２： 回転による影響をなくす\n",
            "\n",
            "本記事では、上記の課題について、PointNetがどのようなアプローチを取ったかを解説する。\n",
            "\n",
            "PointNetの構造\n",
            "まずは、ネットワーク全体の構成について。\n",
            "詳しい説明は省略するが、上段でクラス分類を行い、下段ではsemantic segmentationやpart segmentationを行う。\n",
            "\n",
            "\n",
            "特徴その1. 頂点インデックスの順番による影響をなくす\n",
            "\n",
            "参照：Point Cloud Data Using Deep Learning3\n",
            "上図のように、点群データは頂点インデックスを変えても本質的には何も変わらないため、データの順番に左右されないようなモデルを構築する必要がある。\n",
            "そのための方法として、以下の３つが挙げられる。\n",
            "\n",
            "\n",
            "一定の規則に従って、頂点をソートし直す ⇛  高次元空間での並べ替えは必ずしも上手くいかないのでダメ \n",
            "\n",
            "Data augmentation(データ拡張)によって、頂点の順番に影響を受けにくいモデルを作る ⇛  データが爆発的に増えて現実的ではない（頂点がN個あったら、組み合わせはN!通り）\n",
            "\n",
            "対称関数を使ったモデルを組む ⇛  対称関数(symmetric function)とは...？\n",
            "\n",
            "\n",
            "対称関数とは\n",
            "対称関数とは、変数の順番を入れ替えても値が変わらない関数のこと。\n",
            "例として、以下のような関数がある。\n",
            "\\begin{aligned}\n",
            "f(x_1, x_2, ..., x_n) &= x_1 + x_2 + \\cdots + x_n \\\\\n",
            "f(x_1, x_2, ..., x_n) &= \\max\\{x_1, x_2, ..., x_n\\}\n",
            "\\end{aligned}\n",
            "\n",
            "例えば、$x_1$と$x_2$を入れ替えても、値は変わらないので、対称関数と言える。\n",
            "PointNetではこの対称関数の考え方を応用しており、全ての頂点に対して同じ重みの多層パーセプトロンをかけたり、プーリング層ではMax poolingを行ったりすることで全ての頂点を平等に扱うことに成功している。\n",
            "\n",
            "特徴その2. 回転による影響をなくす\n",
            "\"T-Net\"と呼ばれるネットワークを組み込むことで、アフィン変換行列を推定し、その変換行列をかけることで回転による影響をなくすようにしている。\n",
            "\n",
            "\n",
            "まとめ\n",
            "最近の点群処理でよく使われる深層学習モデル PointNet について、解説した。\n",
            "対称関数やアフィン変換行列を用いた点群処理モデル特有の工夫を説明した。\n",
            "PointNetは、空間の細かい情報が抜けやすかったり、点密度を均一にすると精度が落ちたりといった欠点があったが、それを解決する手法 PointNet++ も提案されているので、興味があればぜひ論文を読んでいただきたい。\n",
            "\n",
            "おわりに\n",
            "ヒトやモノをデータ化＆解析してみたい、という方。\n",
            "３D技術と深層学習を組み合わせて、何か面白いサービスを作ってみたい！、という方。\n",
            "弊社では一緒に働いてくれる仲間を大募集しています。\n",
            "ご興味がある方は下記リンクから是非ご応募ください！\n",
            "https://about.sapeet.com/recruit/\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "PointNet: Deep Learning on Point Sets for 3D Classification and Segmentation ↩\n",
            "\n",
            "\n",
            "VoxNet: A 3D Convolutional Neural Network for Real-Time Object Recognition ↩\n",
            "\n",
            "\n",
            "Point Cloud Data Using Deep Learning ↩\n",
            "\n",
            "\n",
            "\n",
            "Register as a new user and use Qiita more convenientlyYou get articles that match your needsYou can efficiently read back useful informationWhat you can do with signing upSign upLogin155160more_horiz\n",
            "\n",
            "How developers code is here.© 2011-2023Qiita Inc.Guide & HelpAboutTermsPrivacyGuidelineDesign GuidelineFeedbackHelpAdvertisementContentsRelease NoteOfficial EventOfficial ColumnOpportunitiesAdvent CalendarQiita AwardAPISNS@Qiita@qiita_milestone@qiitapoi@QiitaOur serviceQiita TeamQiita JobsQiita ZineOfficial ShopCompanyAbout UsCareersQiita Blog\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# MMR(Maximal Marginal Relevance)\n",
        "# 類似度が高いものを取得したいがなるべく多様な結果も取得したいときに使う\n",
        "found_docs = qdrant.max_marginal_relevance_search(query, k=2, fetch_k=10)\n",
        "print(found_docs[0].page_content)"
      ],
      "metadata": {
        "id": "4u5qPnOj62P2",
        "outputId": "2d30ac27-68f1-4355-de2e-0a140521fd8a",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 44,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "点群×ディープラーニング【入門】 - Qiita\n",
            "searchsearchLoginSignupTrendQuestionOfficial EventOfficial ColumnOpportunitiesOrganization\n",
            "\n",
            "\n",
            "【6/16(金)@渋谷】エンジニア職限定採用イベント「【25卒向け】Qiita Career Meetup for STUDENT」🚀close155160more_horizinfoMore than 3 years have passed since last update.@KYoshiyama(吉山 恭平)posted at 2019-06-05updated at 2019-06-05点群×ディープラーニング【入門】sellPython,MachineLearning,AI,CNN,PointCloud\n",
            "\n",
            "はじめに\n",
            "現在主流となっている点群処理モデル「PointNet」について解説する。\n",
            "（なお、PointNetは2017年に提案されたモデルであり、その後さらに精度の高いモデルが多数提案されている。気になる方はこちらを参照。）\n",
            "また、特に断りのない限り，本記事で使用する画像は、こちらの論文1からの引用。\n",
            "\n",
            "従来手法: ３次元畳み込みニューラルネットワーク\n",
            "Volumetric CNN：点群をボクセル化し、一層一層を画像として扱うことで、畳み込みニューラルネットワークの入力とする。\n",
            "各ボクセルは 2 値で表現し，メッシュからボクセルに変換する際にボクセル内にデータがあるかどうかで 0 か 1 の値を決めて表現する。\n",
            "Volumetric CNNの欠点として、ボクセル化する際に、点群データの空間特性が必然的に変化し、物体の凹凸表現が欠損してしまうことが挙げられる。\n",
            "\n",
            "参照：VoxNet: A 3D Convolutional Neural Network for Real-Time Object Recognition2\n",
            "\n",
            "PointNet\n",
            "\n",
            "PointNetは、クラス分類、セグメンテーション、検出など応用範囲が広く、与えられた点群全体に対してクラス分類を行うこともできるし、点群をセグメントに分割し、それぞれのセグメントに対してラベル付けすることもできる。\n",
            "先述したVolumetric CNNで起きる問題に対し、PointNetは点群の空間特性を最大限残すための独自の点群処理方式を導入した。これにより、従来手法を超えるパフォーマンスを叩き出した。\n",
            "PointNetで提案された点群処理方法の特徴は次の２点である：\n",
            "\n",
            "特徴その１： 頂点インデックスの順番による影響をなくす\n",
            "特徴その２： 回転による影響をなくす\n",
            "\n",
            "本記事では、上記の課題について、PointNetがどのようなアプローチを取ったかを解説する。\n",
            "\n",
            "PointNetの構造\n",
            "まずは、ネットワーク全体の構成について。\n",
            "詳しい説明は省略するが、上段でクラス分類を行い、下段ではsemantic segmentationやpart segmentationを行う。\n",
            "\n",
            "\n",
            "特徴その1. 頂点インデックスの順番による影響をなくす\n",
            "\n",
            "参照：Point Cloud Data Using Deep Learning3\n",
            "上図のように、点群データは頂点インデックスを変えても本質的には何も変わらないため、データの順番に左右されないようなモデルを構築する必要がある。\n",
            "そのための方法として、以下の３つが挙げられる。\n",
            "\n",
            "\n",
            "一定の規則に従って、頂点をソートし直す ⇛  高次元空間での並べ替えは必ずしも上手くいかないのでダメ \n",
            "\n",
            "Data augmentation(データ拡張)によって、頂点の順番に影響を受けにくいモデルを作る ⇛  データが爆発的に増えて現実的ではない（頂点がN個あったら、組み合わせはN!通り）\n",
            "\n",
            "対称関数を使ったモデルを組む ⇛  対称関数(symmetric function)とは...？\n",
            "\n",
            "\n",
            "対称関数とは\n",
            "対称関数とは、変数の順番を入れ替えても値が変わらない関数のこと。\n",
            "例として、以下のような関数がある。\n",
            "\\begin{aligned}\n",
            "f(x_1, x_2, ..., x_n) &= x_1 + x_2 + \\cdots + x_n \\\\\n",
            "f(x_1, x_2, ..., x_n) &= \\max\\{x_1, x_2, ..., x_n\\}\n",
            "\\end{aligned}\n",
            "\n",
            "例えば、$x_1$と$x_2$を入れ替えても、値は変わらないので、対称関数と言える。\n",
            "PointNetではこの対称関数の考え方を応用しており、全ての頂点に対して同じ重みの多層パーセプトロンをかけたり、プーリング層ではMax poolingを行ったりすることで全ての頂点を平等に扱うことに成功している。\n",
            "\n",
            "特徴その2. 回転による影響をなくす\n",
            "\"T-Net\"と呼ばれるネットワークを組み込むことで、アフィン変換行列を推定し、その変換行列をかけることで回転による影響をなくすようにしている。\n",
            "\n",
            "\n",
            "まとめ\n",
            "最近の点群処理でよく使われる深層学習モデル PointNet について、解説した。\n",
            "対称関数やアフィン変換行列を用いた点群処理モデル特有の工夫を説明した。\n",
            "PointNetは、空間の細かい情報が抜けやすかったり、点密度を均一にすると精度が落ちたりといった欠点があったが、それを解決する手法 PointNet++ も提案されているので、興味があればぜひ論文を読んでいただきたい。\n",
            "\n",
            "おわりに\n",
            "ヒトやモノをデータ化＆解析してみたい、という方。\n",
            "３D技術と深層学習を組み合わせて、何か面白いサービスを作ってみたい！、という方。\n",
            "弊社では一緒に働いてくれる仲間を大募集しています。\n",
            "ご興味がある方は下記リンクから是非ご応募ください！\n",
            "https://about.sapeet.com/recruit/\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "PointNet: Deep Learning on Point Sets for 3D Classification and Segmentation ↩\n",
            "\n",
            "\n",
            "VoxNet: A 3D Convolutional Neural Network for Real-Time Object Recognition ↩\n",
            "\n",
            "\n",
            "Point Cloud Data Using Deep Learning ↩\n",
            "\n",
            "\n",
            "\n",
            "Register as a new user and use Qiita more convenientlyYou get articles that match your needsYou can efficiently read back useful informationWhat you can do with signing upSign upLogin155160more_horiz\n",
            "\n",
            "How developers code is here.© 2011-2023Qiita Inc.Guide & HelpAboutTermsPrivacyGuidelineDesign GuidelineFeedbackHelpAdvertisementContentsRelease NoteOfficial EventOfficial ColumnOpportunitiesAdvent CalendarQiita AwardAPISNS@Qiita@qiita_milestone@qiitapoi@QiitaOur serviceQiita TeamQiita JobsQiita ZineOfficial ShopCompanyAbout UsCareersQiita Blog\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "LI5QBfuN9F1h"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3.8.13 64-bit ('kaggle')",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.8.13"
    },
    "orig_nbformat": 4,
    "vscode": {
      "interpreter": {
        "hash": "0efafe6c13c226858b1e3209a708328284172effb51c12dbb0bda90f2bc21738"
      }
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}