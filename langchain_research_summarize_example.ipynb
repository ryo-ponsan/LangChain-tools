{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ryo-ponsan/LangChain-tools/blob/main/langchain_research_summarize_example.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hjgA4K_q-ERo"
      },
      "source": [
        "# 論文の概要を把握するテンプレート（非プログラマー向け）\n",
        "**社内データはくれぐれも入力しないように**お願いします。\n",
        "\n",
        "論文や資料の概要をさらっと掴みたい際に使えると思います。その後、詳細を確認したい場合、元論文を確認する使い方が良いと思います。\n",
        "## ノートブックの流れ\n",
        "1. 各種ライブラリインストール\n",
        "2. 前準備\n",
        "3. 読み込むPDFファイルの選択\n",
        "4. OpenAIのAPI keyの設定\n",
        "5. プロンプトの設定\n",
        "6. 結果出力\n",
        "7. 応用\n",
        "8. LangChainのポテンシャル\n",
        "9. 注意事項\n",
        "10. 参考\n",
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "28dVohy8-Hh2"
      },
      "source": [
        "# 1. 各種ライブラリインストール\n",
        "セルをクリックして、shift+Enterを押し各セルを実行してください。または、ランタイムタブから、「すべてのセルを実行」を選択してください。最初はshift+Enterで徐々に進める方法がおすすめです。"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sfqDl-I8QuIn",
        "outputId": "e72122e0-3284-4cd2-eecd-27bb48f80479"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Writing requirements.txt\n"
          ]
        }
      ],
      "source": [
        "%%writefile requirements.txt\n",
        "openai\n",
        "chromadb\n",
        "langchain\n",
        "pypdf\n",
        "tiktoken"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XC9hMrSsxzof",
        "outputId": "c7b19eed-bbd8-485e-a25e-efacd70c1b29"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting openai (from -r requirements.txt (line 1))\n",
            "  Downloading openai-0.27.6-py3-none-any.whl (71 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m71.9/71.9 kB\u001b[0m \u001b[31m1.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting chromadb (from -r requirements.txt (line 2))\n",
            "  Downloading chromadb-0.3.23-py3-none-any.whl (71 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m71.3/71.3 kB\u001b[0m \u001b[31m2.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting langchain (from -r requirements.txt (line 3))\n",
            "  Downloading langchain-0.0.173-py3-none-any.whl (858 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m858.2/858.2 kB\u001b[0m \u001b[31m16.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting pypdf (from -r requirements.txt (line 4))\n",
            "  Downloading pypdf-3.8.1-py3-none-any.whl (248 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m248.8/248.8 kB\u001b[0m \u001b[31m12.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting tiktoken (from -r requirements.txt (line 5))\n",
            "  Downloading tiktoken-0.4.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.7 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.7/1.7 MB\u001b[0m \u001b[31m35.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: requests>=2.20 in /usr/local/lib/python3.10/dist-packages (from openai->-r requirements.txt (line 1)) (2.27.1)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from openai->-r requirements.txt (line 1)) (4.65.0)\n",
            "Collecting aiohttp (from openai->-r requirements.txt (line 1))\n",
            "  Downloading aiohttp-3.8.4-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.0 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.0/1.0 MB\u001b[0m \u001b[31m22.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: pandas>=1.3 in /usr/local/lib/python3.10/dist-packages (from chromadb->-r requirements.txt (line 2)) (1.5.3)\n",
            "Collecting requests>=2.20 (from openai->-r requirements.txt (line 1))\n",
            "  Downloading requests-2.30.0-py3-none-any.whl (62 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m62.5/62.5 kB\u001b[0m \u001b[31m2.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: pydantic>=1.9 in /usr/local/lib/python3.10/dist-packages (from chromadb->-r requirements.txt (line 2)) (1.10.7)\n",
            "Collecting hnswlib>=0.7 (from chromadb->-r requirements.txt (line 2))\n",
            "  Downloading hnswlib-0.7.0.tar.gz (33 kB)\n",
            "  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting clickhouse-connect>=0.5.7 (from chromadb->-r requirements.txt (line 2))\n",
            "  Downloading clickhouse_connect-0.5.24-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (922 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m922.6/922.6 kB\u001b[0m \u001b[31m65.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting sentence-transformers>=2.2.2 (from chromadb->-r requirements.txt (line 2))\n",
            "  Downloading sentence-transformers-2.2.2.tar.gz (85 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m86.0/86.0 kB\u001b[0m \u001b[31m9.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: duckdb>=0.7.1 in /usr/local/lib/python3.10/dist-packages (from chromadb->-r requirements.txt (line 2)) (0.7.1)\n",
            "Collecting fastapi>=0.85.1 (from chromadb->-r requirements.txt (line 2))\n",
            "  Downloading fastapi-0.95.2-py3-none-any.whl (56 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m57.0/57.0 kB\u001b[0m \u001b[31m6.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting uvicorn[standard]>=0.18.3 (from chromadb->-r requirements.txt (line 2))\n",
            "  Downloading uvicorn-0.22.0-py3-none-any.whl (58 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m58.3/58.3 kB\u001b[0m \u001b[31m2.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: numpy>=1.21.6 in /usr/local/lib/python3.10/dist-packages (from chromadb->-r requirements.txt (line 2)) (1.22.4)\n",
            "Collecting posthog>=2.4.0 (from chromadb->-r requirements.txt (line 2))\n",
            "  Downloading posthog-3.0.1-py2.py3-none-any.whl (37 kB)\n",
            "Requirement already satisfied: typing-extensions>=4.5.0 in /usr/local/lib/python3.10/dist-packages (from chromadb->-r requirements.txt (line 2)) (4.5.0)\n",
            "Requirement already satisfied: PyYAML>=5.4.1 in /usr/local/lib/python3.10/dist-packages (from langchain->-r requirements.txt (line 3)) (6.0)\n",
            "Requirement already satisfied: SQLAlchemy<3,>=1.4 in /usr/local/lib/python3.10/dist-packages (from langchain->-r requirements.txt (line 3)) (2.0.10)\n",
            "Collecting async-timeout<5.0.0,>=4.0.0 (from langchain->-r requirements.txt (line 3))\n",
            "  Downloading async_timeout-4.0.2-py3-none-any.whl (5.8 kB)\n",
            "Collecting dataclasses-json<0.6.0,>=0.5.7 (from langchain->-r requirements.txt (line 3))\n",
            "  Downloading dataclasses_json-0.5.7-py3-none-any.whl (25 kB)\n",
            "Requirement already satisfied: numexpr<3.0.0,>=2.8.4 in /usr/local/lib/python3.10/dist-packages (from langchain->-r requirements.txt (line 3)) (2.8.4)\n",
            "Collecting openapi-schema-pydantic<2.0,>=1.2 (from langchain->-r requirements.txt (line 3))\n",
            "  Downloading openapi_schema_pydantic-1.2.4-py3-none-any.whl (90 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m90.0/90.0 kB\u001b[0m \u001b[31m10.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: tenacity<9.0.0,>=8.1.0 in /usr/local/lib/python3.10/dist-packages (from langchain->-r requirements.txt (line 3)) (8.2.2)\n",
            "Requirement already satisfied: regex>=2022.1.18 in /usr/local/lib/python3.10/dist-packages (from tiktoken->-r requirements.txt (line 5)) (2022.10.31)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->openai->-r requirements.txt (line 1)) (23.1.0)\n",
            "Requirement already satisfied: charset-normalizer<4.0,>=2.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->openai->-r requirements.txt (line 1)) (2.0.12)\n",
            "Collecting multidict<7.0,>=4.5 (from aiohttp->openai->-r requirements.txt (line 1))\n",
            "  Downloading multidict-6.0.4-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (114 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m114.5/114.5 kB\u001b[0m \u001b[31m11.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting yarl<2.0,>=1.0 (from aiohttp->openai->-r requirements.txt (line 1))\n",
            "  Downloading yarl-1.9.2-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (268 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m268.8/268.8 kB\u001b[0m \u001b[31m24.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting frozenlist>=1.1.1 (from aiohttp->openai->-r requirements.txt (line 1))\n",
            "  Downloading frozenlist-1.3.3-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (149 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m149.6/149.6 kB\u001b[0m \u001b[31m16.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting aiosignal>=1.1.2 (from aiohttp->openai->-r requirements.txt (line 1))\n",
            "  Downloading aiosignal-1.3.1-py3-none-any.whl (7.6 kB)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.10/dist-packages (from clickhouse-connect>=0.5.7->chromadb->-r requirements.txt (line 2)) (2022.12.7)\n",
            "Requirement already satisfied: urllib3>=1.26 in /usr/local/lib/python3.10/dist-packages (from clickhouse-connect>=0.5.7->chromadb->-r requirements.txt (line 2)) (1.26.15)\n",
            "Requirement already satisfied: pytz in /usr/local/lib/python3.10/dist-packages (from clickhouse-connect>=0.5.7->chromadb->-r requirements.txt (line 2)) (2022.7.1)\n",
            "Collecting zstandard (from clickhouse-connect>=0.5.7->chromadb->-r requirements.txt (line 2))\n",
            "  Downloading zstandard-0.21.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (2.7 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.7/2.7 MB\u001b[0m \u001b[31m89.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting lz4 (from clickhouse-connect>=0.5.7->chromadb->-r requirements.txt (line 2))\n",
            "  Downloading lz4-4.3.2-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.3 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.3/1.3 MB\u001b[0m \u001b[31m72.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting marshmallow<4.0.0,>=3.3.0 (from dataclasses-json<0.6.0,>=0.5.7->langchain->-r requirements.txt (line 3))\n",
            "  Downloading marshmallow-3.19.0-py3-none-any.whl (49 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m49.1/49.1 kB\u001b[0m \u001b[31m5.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting marshmallow-enum<2.0.0,>=1.5.1 (from dataclasses-json<0.6.0,>=0.5.7->langchain->-r requirements.txt (line 3))\n",
            "  Downloading marshmallow_enum-1.5.1-py2.py3-none-any.whl (4.2 kB)\n",
            "Collecting typing-inspect>=0.4.0 (from dataclasses-json<0.6.0,>=0.5.7->langchain->-r requirements.txt (line 3))\n",
            "  Downloading typing_inspect-0.8.0-py3-none-any.whl (8.7 kB)\n",
            "Collecting starlette<0.28.0,>=0.27.0 (from fastapi>=0.85.1->chromadb->-r requirements.txt (line 2))\n",
            "  Downloading starlette-0.27.0-py3-none-any.whl (66 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m67.0/67.0 kB\u001b[0m \u001b[31m7.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: python-dateutil>=2.8.1 in /usr/local/lib/python3.10/dist-packages (from pandas>=1.3->chromadb->-r requirements.txt (line 2)) (2.8.2)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from posthog>=2.4.0->chromadb->-r requirements.txt (line 2)) (1.16.0)\n",
            "Collecting monotonic>=1.5 (from posthog>=2.4.0->chromadb->-r requirements.txt (line 2))\n",
            "  Downloading monotonic-1.6-py2.py3-none-any.whl (8.2 kB)\n",
            "Collecting backoff>=1.10.0 (from posthog>=2.4.0->chromadb->-r requirements.txt (line 2))\n",
            "  Downloading backoff-2.2.1-py3-none-any.whl (15 kB)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests>=2.20->openai->-r requirements.txt (line 1)) (3.4)\n",
            "Collecting transformers<5.0.0,>=4.6.0 (from sentence-transformers>=2.2.2->chromadb->-r requirements.txt (line 2))\n",
            "  Downloading transformers-4.29.2-py3-none-any.whl (7.1 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.1/7.1 MB\u001b[0m \u001b[31m97.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: torch>=1.6.0 in /usr/local/lib/python3.10/dist-packages (from sentence-transformers>=2.2.2->chromadb->-r requirements.txt (line 2)) (2.0.0+cu118)\n",
            "Requirement already satisfied: torchvision in /usr/local/lib/python3.10/dist-packages (from sentence-transformers>=2.2.2->chromadb->-r requirements.txt (line 2)) (0.15.1+cu118)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.10/dist-packages (from sentence-transformers>=2.2.2->chromadb->-r requirements.txt (line 2)) (1.2.2)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.10/dist-packages (from sentence-transformers>=2.2.2->chromadb->-r requirements.txt (line 2)) (1.10.1)\n",
            "Requirement already satisfied: nltk in /usr/local/lib/python3.10/dist-packages (from sentence-transformers>=2.2.2->chromadb->-r requirements.txt (line 2)) (3.8.1)\n",
            "Collecting sentencepiece (from sentence-transformers>=2.2.2->chromadb->-r requirements.txt (line 2))\n",
            "  Downloading sentencepiece-0.1.99-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.3 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.3/1.3 MB\u001b[0m \u001b[31m78.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting huggingface-hub>=0.4.0 (from sentence-transformers>=2.2.2->chromadb->-r requirements.txt (line 2))\n",
            "  Downloading huggingface_hub-0.14.1-py3-none-any.whl (224 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m224.5/224.5 kB\u001b[0m \u001b[31m26.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: greenlet!=0.4.17 in /usr/local/lib/python3.10/dist-packages (from SQLAlchemy<3,>=1.4->langchain->-r requirements.txt (line 3)) (2.0.2)\n",
            "Requirement already satisfied: click>=7.0 in /usr/local/lib/python3.10/dist-packages (from uvicorn[standard]>=0.18.3->chromadb->-r requirements.txt (line 2)) (8.1.3)\n",
            "Collecting h11>=0.8 (from uvicorn[standard]>=0.18.3->chromadb->-r requirements.txt (line 2))\n",
            "  Downloading h11-0.14.0-py3-none-any.whl (58 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m58.3/58.3 kB\u001b[0m \u001b[31m6.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting httptools>=0.5.0 (from uvicorn[standard]>=0.18.3->chromadb->-r requirements.txt (line 2))\n",
            "  Downloading httptools-0.5.0-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (414 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m414.1/414.1 kB\u001b[0m \u001b[31m41.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting python-dotenv>=0.13 (from uvicorn[standard]>=0.18.3->chromadb->-r requirements.txt (line 2))\n",
            "  Downloading python_dotenv-1.0.0-py3-none-any.whl (19 kB)\n",
            "Collecting uvloop!=0.15.0,!=0.15.1,>=0.14.0 (from uvicorn[standard]>=0.18.3->chromadb->-r requirements.txt (line 2))\n",
            "  Downloading uvloop-0.17.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (4.1 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m4.1/4.1 MB\u001b[0m \u001b[31m71.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting watchfiles>=0.13 (from uvicorn[standard]>=0.18.3->chromadb->-r requirements.txt (line 2))\n",
            "  Downloading watchfiles-0.19.0-cp37-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.3 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.3/1.3 MB\u001b[0m \u001b[31m76.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting websockets>=10.4 (from uvicorn[standard]>=0.18.3->chromadb->-r requirements.txt (line 2))\n",
            "  Downloading websockets-11.0.3-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (129 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m129.9/129.9 kB\u001b[0m \u001b[31m13.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.4.0->sentence-transformers>=2.2.2->chromadb->-r requirements.txt (line 2)) (3.12.0)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.4.0->sentence-transformers>=2.2.2->chromadb->-r requirements.txt (line 2)) (2023.4.0)\n",
            "Requirement already satisfied: packaging>=20.9 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.4.0->sentence-transformers>=2.2.2->chromadb->-r requirements.txt (line 2)) (23.1)\n",
            "Requirement already satisfied: anyio<5,>=3.4.0 in /usr/local/lib/python3.10/dist-packages (from starlette<0.28.0,>=0.27.0->fastapi>=0.85.1->chromadb->-r requirements.txt (line 2)) (3.6.2)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch>=1.6.0->sentence-transformers>=2.2.2->chromadb->-r requirements.txt (line 2)) (1.11.1)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch>=1.6.0->sentence-transformers>=2.2.2->chromadb->-r requirements.txt (line 2)) (3.1)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch>=1.6.0->sentence-transformers>=2.2.2->chromadb->-r requirements.txt (line 2)) (3.1.2)\n",
            "Requirement already satisfied: triton==2.0.0 in /usr/local/lib/python3.10/dist-packages (from torch>=1.6.0->sentence-transformers>=2.2.2->chromadb->-r requirements.txt (line 2)) (2.0.0)\n",
            "Requirement already satisfied: cmake in /usr/local/lib/python3.10/dist-packages (from triton==2.0.0->torch>=1.6.0->sentence-transformers>=2.2.2->chromadb->-r requirements.txt (line 2)) (3.25.2)\n",
            "Requirement already satisfied: lit in /usr/local/lib/python3.10/dist-packages (from triton==2.0.0->torch>=1.6.0->sentence-transformers>=2.2.2->chromadb->-r requirements.txt (line 2)) (16.0.3)\n",
            "Collecting tokenizers!=0.11.3,<0.14,>=0.11.1 (from transformers<5.0.0,>=4.6.0->sentence-transformers>=2.2.2->chromadb->-r requirements.txt (line 2))\n",
            "  Downloading tokenizers-0.13.3-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (7.8 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.8/7.8 MB\u001b[0m \u001b[31m98.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting mypy-extensions>=0.3.0 (from typing-inspect>=0.4.0->dataclasses-json<0.6.0,>=0.5.7->langchain->-r requirements.txt (line 3))\n",
            "  Downloading mypy_extensions-1.0.0-py3-none-any.whl (4.7 kB)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.10/dist-packages (from nltk->sentence-transformers>=2.2.2->chromadb->-r requirements.txt (line 2)) (1.2.0)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn->sentence-transformers>=2.2.2->chromadb->-r requirements.txt (line 2)) (3.1.0)\n",
            "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /usr/local/lib/python3.10/dist-packages (from torchvision->sentence-transformers>=2.2.2->chromadb->-r requirements.txt (line 2)) (8.4.0)\n",
            "Requirement already satisfied: sniffio>=1.1 in /usr/local/lib/python3.10/dist-packages (from anyio<5,>=3.4.0->starlette<0.28.0,>=0.27.0->fastapi>=0.85.1->chromadb->-r requirements.txt (line 2)) (1.3.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch>=1.6.0->sentence-transformers>=2.2.2->chromadb->-r requirements.txt (line 2)) (2.1.2)\n",
            "Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->torch>=1.6.0->sentence-transformers>=2.2.2->chromadb->-r requirements.txt (line 2)) (1.3.0)\n",
            "Building wheels for collected packages: hnswlib, sentence-transformers\n",
            "  Building wheel for hnswlib (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for hnswlib: filename=hnswlib-0.7.0-cp310-cp310-linux_x86_64.whl size=2119876 sha256=d32d5385ef11d9d3faf8e2a3d94a361dfa836121ec21ffa0b764e72214eaa993\n",
            "  Stored in directory: /root/.cache/pip/wheels/8a/ae/ec/235a682e0041fbaeee389843670581ec6c66872db856dfa9a4\n",
            "  Building wheel for sentence-transformers (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for sentence-transformers: filename=sentence_transformers-2.2.2-py3-none-any.whl size=125926 sha256=8752893d5729e0491cd9393f93de8beea6e937fa3ec9353740be2ef27363072c\n",
            "  Stored in directory: /root/.cache/pip/wheels/62/f2/10/1e606fd5f02395388f74e7462910fe851042f97238cbbd902f\n",
            "Successfully built hnswlib sentence-transformers\n",
            "Installing collected packages: tokenizers, sentencepiece, monotonic, zstandard, websockets, uvloop, requests, python-dotenv, pypdf, mypy-extensions, multidict, marshmallow, lz4, httptools, hnswlib, h11, frozenlist, backoff, async-timeout, yarl, watchfiles, uvicorn, typing-inspect, tiktoken, starlette, posthog, openapi-schema-pydantic, marshmallow-enum, huggingface-hub, clickhouse-connect, aiosignal, transformers, fastapi, dataclasses-json, aiohttp, openai, langchain, sentence-transformers, chromadb\n",
            "  Attempting uninstall: requests\n",
            "    Found existing installation: requests 2.27.1\n",
            "    Uninstalling requests-2.27.1:\n",
            "      Successfully uninstalled requests-2.27.1\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "google-colab 1.0.0 requires requests==2.27.1, but you have requests 2.30.0 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed aiohttp-3.8.4 aiosignal-1.3.1 async-timeout-4.0.2 backoff-2.2.1 chromadb-0.3.23 clickhouse-connect-0.5.24 dataclasses-json-0.5.7 fastapi-0.95.2 frozenlist-1.3.3 h11-0.14.0 hnswlib-0.7.0 httptools-0.5.0 huggingface-hub-0.14.1 langchain-0.0.173 lz4-4.3.2 marshmallow-3.19.0 marshmallow-enum-1.5.1 monotonic-1.6 multidict-6.0.4 mypy-extensions-1.0.0 openai-0.27.6 openapi-schema-pydantic-1.2.4 posthog-3.0.1 pypdf-3.8.1 python-dotenv-1.0.0 requests-2.30.0 sentence-transformers-2.2.2 sentencepiece-0.1.99 starlette-0.27.0 tiktoken-0.4.0 tokenizers-0.13.3 transformers-4.29.2 typing-inspect-0.8.0 uvicorn-0.22.0 uvloop-0.17.0 watchfiles-0.19.0 websockets-11.0.3 yarl-1.9.2 zstandard-0.21.0\n"
          ]
        }
      ],
      "source": [
        "%pip install -r requirements.txt"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HIY0cyY9-VBg"
      },
      "source": [
        "# 2. 前準備\n",
        "インストールが完了したら、各種ライブラリをインポートします。これでChatGPTやLangChainの機能を呼び出すことができます。"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0I03xUvkx5HG"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import platform\n",
        "\n",
        "import openai\n",
        "import chromadb\n",
        "import langchain\n",
        "\n",
        "from langchain.embeddings.openai import OpenAIEmbeddings\n",
        "from langchain.vectorstores import Chroma\n",
        "from langchain.text_splitter import CharacterTextSplitter\n",
        "from langchain.chat_models import ChatOpenAI\n",
        "from langchain.chains import ConversationalRetrievalChain\n",
        "from langchain.document_loaders import PyPDFLoader"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nHgnt-gMAcGF"
      },
      "source": [
        "GoogleDriveをマウントします。GoogleColabからGoogleDriveのフォルダにアクセスできるようにします。実行されると確認画面が表示されるので承認してください。"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3PC2iv3lyKiF",
        "outputId": "c4df1a62-20e9-4de3-a1ca-a94dbef05a4e"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Au2HRBKpAqR5"
      },
      "source": [
        "# 3. 読み込むPDFファイルの選択\n",
        "下記の場合、マイドライブ直下のColab Notebooks直下に、datasetフォルダを事前に作成し、PDFファイルを保存しています。**各自変更変えてください。**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oIt71LDoyQ11"
      },
      "outputs": [],
      "source": [
        "loader = PyPDFLoader(\"/content/drive/My Drive/Colab Notebooks/world_model_dataset/Modeling Task Uncertainty for Safe Meta-Imitation Learning(2022).pdf\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "neTT6AUbyjQI"
      },
      "outputs": [],
      "source": [
        "pages = loader.load_and_split()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dpZt57NFBDMl"
      },
      "source": [
        "# 4. OpenAIのAPI key設定\n",
        "[OpenAI API key](https://platform.openai.com/account/api-keys)を取得し、`os.environ[\"OPENAI_API_KEY\"] = 'sk-?????????????????????????????????'`の`sk-?????????????????????????????????`を書き換えてください。  尚API keyは誰にも共有しないよう注意してください。\n",
        "発行手順は[こちら](https://auto-worker.com/blog/?p=6988)を参考にしてください。\n",
        "\n",
        "## 注意事項  \n",
        "OpenAI API key発行ページの「Usage」タブから、OpenAI APIの使用状況、使用期限が確認できます。使用上限、使用期限が過ぎていた場合、同アカウントで使用するには課金してください。"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wvQ3osMIym0m"
      },
      "outputs": [],
      "source": [
        "os.environ[\"OPENAI_API_KEY\"] = 'sk-xxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxx'\n",
        "openai.api_key = os.getenv(\"OPENAI_API_KEY\")\n",
        "llm = ChatOpenAI(temperature=0, model_name=\"gpt-3.5-turbo\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Wy2WRXBSzGPt",
        "outputId": "4370ccae-ddcb-4b11-d474-d87e1be3b49d"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "WARNING:chromadb:Using embedded DuckDB with persistence: data will be stored in: .\n"
          ]
        }
      ],
      "source": [
        "embeddings = OpenAIEmbeddings()\n",
        "vectorstore = Chroma.from_documents(pages, embedding=embeddings, persist_directory=\".\")\n",
        "vectorstore.persist()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cTt_hnDXzmqm"
      },
      "outputs": [],
      "source": [
        "pdf_qa = ConversationalRetrievalChain.from_llm(llm, vectorstore.as_retriever(), return_source_documents=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EFjkJwMjJ9_L"
      },
      "source": [
        "# 5. プロンプト設定\n",
        "PDFから、抽出したり、要約したり、chatGPTに指示したい項目を入力してください。今回の例の場合下記内容を順番に抽出します。\n",
        "1. {title}の概要\n",
        "2. {title}の原理\n",
        "3. {title}の新規性\n",
        "4. {title}の課題、将来性  \n",
        "\n",
        "尚queryは聞きたい項目だけ増やすことが可能です。今回のテンプレートの場合、先行研究を聞くプロンプトquery5 を用意した場合は、`query_list = [query1, query2, query3, query4, query5]` と`query_title = [\"概要\", \"原理\", \"新規性\", \"課題等\",\"先行研究\"]`と設定しなおすとスムーズです。\n",
        "\n",
        "また事例では、`MVLidarNet`という単語を使いまわしたかったため、`title`変数を用意していますが、単語不要な場合は、下記のような内容にすれば汎用性が高いと思います。\n",
        "\n",
        "```\n",
        "query1 = \"Please give us an overview of this reserch? Answer in Japanese.\"\n",
        "query2 = \"Please tell me the technical principles of featured technology in this paper step by step in list form.Answer in Japanese.\"\n",
        "query3 = \"Please let me know if there is any description about the novelty of featured technology in this paper.Answer in Japanese.\"\n",
        "query4 = \"Please tell me step-by-step if there are any descriptions about featured technology in this paper issues and future prospects.Answer in Japanese.\"\n",
        "```"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xdgeq3cmz9Ye"
      },
      "outputs": [],
      "source": [
        "# title = \"NVRaderNet\"\n",
        "\n",
        "# query1 = f\"Please give us an overview of the {title}? Answer in Japanese.\"\n",
        "# query2 = f\"Please tell me the technical principles of the {title} step by step in list form.Answer in Japanese.\"\n",
        "# query3 = f\"Please let me know if there is any description about the novelty of the {title} technology.Answer in Japanese.\"\n",
        "# query4 = f\"Please tell me step-by-step if there are any descriptions about the {title} technology problems and future prospects.Answer in Japanese.\"\n",
        "query0 = \"Why did they start this reserch?If this paper describes the issues related to the technology of this genre in the past, the issues and problems of previous research and previous research, and the significance of conducting this research, please extract information. step by step in list form. Answer in Japanese step by step in list form\"\n",
        "query1 = \"Please give us an overview of this reserch step by step in list form. Answer in Japanese step by step in list form.\"\n",
        "query2 = \"Please tell me the technical principles of featured technology in this paper step by step in list form. Answer in Japanese step by step in list form.\"\n",
        "query3 = \"Please let me know if there is any description about the novelty of featured technology in this paper step by step in list form. Answer in Japanese.\"\n",
        "query4 = \"Please tell me if there are any descriptions about issues of featured technology in this paper and future prospects step by step in list form.Answer in Japanese.\"\n",
        "query5 = \"Please tell me if there are any conclusion of featured technology in this paper and future prospects step by step in list form.Answer in Japanese within 50 words.\"\n",
        "\n",
        "query_list = [query0, query1, query2, query3, query4, query5]\n",
        "query_title = [\"背景\",\"概要\", \"原理\", \"新規性\", \"課題等\",\"結論\"]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0yGFsP2QDTJV"
      },
      "source": [
        "# 6. 結果出力\n",
        "PDFに記載されてない内容に関するプロンプトやあいまいなプロンプトを投げると、嘘(ハルシネーション)をいう可能性があります。そういった場合は、情報がない場合の回答方法を指示したり、ルールを設定することで回避可能です。"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HyiKBd85DSoY",
        "outputId": "311f96fc-0477-4402-af6f-27481fbf8fdb"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "############\n",
            "1 : 背景\n",
            "############\n",
            "1. ロボットの汎用性の開発は、ロボット工学における重要な課題である。\n",
            "2. ロボットが多様な環境で幅広いタスクを実行できるようにするため、データ駆動型アプローチが注目されている。\n",
            "3. 特に、メタラーニングは、ロボット制御のメタラーニングの研究において、新しいタスクを解決するために他のタスクの経験を活用することができることが示されている。\n",
            "4. しかし、これまでの研究では、パフォーマンスの向上に焦点が当てられており、安全性の問題は十分に探究されていない。\n",
            "5. 本研究では、メタラーニングにおけるタスク不確実性と安全性の関係を明らかにし、安全なメタイミテーション学習のためのタスク不確実性の推定フレームワークを提案する。\n",
            "############\n",
            "2 : 概要\n",
            "############\n",
            "1. ロボットの柔軟性を高めるために、経験データからコントローラを学習することが有望なアプローチである。\n",
            "2. メタ学習は、ロボットの制御を学習するための最も有望な新しい手法の1つであり、転移学習の中でも注目されている。\n",
            "3. メタ学習において、タスクの不確実性を評価することは、安全なロボット学習システムの展開にとって重要である。\n",
            "4. 本研究では、PETNetと呼ばれる新しいフレームワークを提案し、確率的タスク埋め込み空間でタスクの不確実性を推定することができる。\n",
            "5. PETNetは、視覚的な模倣学習のためのタスク不確実性の推定において、良好な性能を発揮することが示された。\n",
            "6. PETNetは、先行研究の方法と比較して、メタテストにおいて同等またはより高いパフォーマンスを達成することができる。\n",
            "7. PETNetは、異常なデモンストレーションや合成されたデモンストレーションに対しても、タスクの不確実性を捉えることができる。\n",
            "8. PETNetは、ロボット学習システムの安全な展開に向けた重要な一歩である。\n",
            "############\n",
            "3 : 原理\n",
            "############\n",
            "1. この論文の特徴的な技術は、PETNetと呼ばれるタスク不確実性を推定するためのフレームワークである。\n",
            "2. PETNetは、タスク埋め込み空間で確率的推論を行うことにより、タスク不確実性を推定する。\n",
            "3. PETNetは、メタ模倣学習のベンチマーク手順に従って評価され、以前の手法と同等またはそれ以上の性能を発揮することが示されている。\n",
            "4. PETNetは、不適切なデモンストレーションや生成された分布外のデモンストレーションを使用してテストされ、与えられたデモンストレーションに固有のタスクの不確実性を捕捉する能力があることが示されている。\n",
            "5. PETNetによって推定されたタスクの不確実性は、コントローラーの安全性を向上させるために役立つことが示されている。\n",
            "############\n",
            "4 : 新規性\n",
            "############\n",
            "申し訳ありませんが、この論文の技術の新規性についてステップバイステップでリスト形式で説明されている部分はありません。論文全体を読んで、新規性に関する情報を収集する必要があります。\n",
            "############\n",
            "5 : 課題等\n",
            "############\n",
            "この論文において、特徴的な技術の問題点や今後の展望についての記述は以下のようになっています。\n",
            "\n",
            "問題点：\n",
            "- メタ学習における安全性の問題が未解決である。\n",
            "- デモンストレーションに含まれるタスクの不確実性を考慮しない場合、コントローラが適切に機能しない可能性がある。\n",
            "\n",
            "展望：\n",
            "- 提案手法PETNetは、タスクの不確実性を考慮することで、より安全なメタ学習を実現することができる。\n",
            "- PETNetは、他の学習アルゴリズムにも拡張可能であり、リアルタイムでの応用も可能である。\n",
            "- PETNetを用いたメタ学習は、より多様なタスクに適用することができるようになる可能性がある。\n",
            "############\n",
            "6 : 結論\n",
            "############\n",
            "1. 研究の目的は、メタイミテーション学習におけるタスク不確実性の評価の重要性を指摘することであった。\n",
            "2. PETNetという新しいフレームワークを提案し、確率的タスク埋め込み空間を使用してタスク不確実性を測定することができることを示した。\n",
            "3. PETNetは、以前の方法よりもシミュレーションされた押しタスクのベンチマークで優れたパフォーマンスを発揮し、タスクの不確実性を追跡することができることを示した。\n",
            "4. PETNetの拡張として、強化学習や逆強化学習などの他の学習アルゴリズムに適用することができる。\n",
            "5. PETNetの評価されたタスク不確実性をメタトレーニングに活用することができる。\n",
            "6. PETNetは、言語指示、人間のデモンストレーター、クラウドソーシングサービスなど、他のモダリティのデモンストレーションを活用することができる。\n",
            "7. PETNetは、カリキュラム学習やアクティブラーニングなどの形で拡張することができる。\n",
            "8. PETNetの将来の研究として、より多様なタスクにスケーリングすること、リアルロボットへの展開、タスク不確実性を活用したメタトレーニングの最適化などが挙げられる。\n"
          ]
        }
      ],
      "source": [
        "all_result = \"\"\n",
        "for i, query in enumerate(query_list):\n",
        "    chat_history = []\n",
        "\n",
        "    result = pdf_qa({\"question\": query, \"chat_history\": chat_history})\n",
        "    print(\"############\")\n",
        "    print(f\"{i+1} : {query_title[i]}\")\n",
        "    print(\"############\")\n",
        "    print(result[\"answer\"])\n",
        "    all_result += result[\"answer\"]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "F9_ZNw4ynni4"
      },
      "source": [
        "## 6-2. パワポ用に出力を修正(テスト用)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "O_fwas2vn2dr"
      },
      "outputs": [],
      "source": [
        "# query = f\"Summarize the contents enclosed in ``` ``` in {len(query_title)} chapter format step by step in list form by using number.```{all_result}``` Answer in Japanese.\"\n",
        "\n",
        "# chat_history = []\n",
        "\n",
        "# result = pdf_qa({\"question\": query, \"chat_history\": chat_history})\n",
        "\n",
        "# print(result[\"answer\"])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-Uh-5Goka7gG"
      },
      "source": [
        "# 7. 応用\n",
        "\n",
        "上記は、それぞれが独立したqueryになっていましたが、過去の質疑回答を踏まえた上で、chatGPTが回答する方法や、chatBotのような対話形式で形で回答する方法、など様々な方法があります。下記過去の質疑回答を踏まえた上で回答させる一例です。試したい方は、下の方にコードとして書き込んでみてください。PDFは最初に読み込んだものを参照しています。[こちらの記事](https://zenn.dev/umi_mori/books/prompt-engineer/viewer/langchain_overview)も参考になります。\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "### 1回目のやり取り\n",
        "\n",
        "```\n",
        "\n",
        "query = \"what's the main purpose of this article? Answer in Japanese.\"\n",
        "\n",
        "chat_history = []\n",
        "\n",
        "result = pdf_qa({\"question\": query, \"chat_history\": chat_history})\n",
        "\n",
        "print(result[\"answer\"])\n",
        "\n",
        "```\n",
        "\n",
        "### 1回目に対する2回目のやり取り\n",
        "\n",
        "```\n",
        "\n",
        "query2 = \"Please tell me more in detail.Answer in Japanese.\"\n",
        "chat_history1 = [(query, result[\"answer\"])]\n",
        "\n",
        "result2 = pdf_qa({\"question\": query2, \"chat_history\": chat_history1})\n",
        "\n",
        "print(result2[\"answer\"])\n",
        "\n",
        "\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GOaPJvxwIul8"
      },
      "source": [
        "# 8.LangChainのポテンシャル\n",
        "PDFをchatGPTに読ませた上で、情報抽出することができました。ほかにもchatGPTや他のLLMモデルにGoogle検索を使ってウェブスクレイピングをさせたり、chatGPTとの対話記録を保持するデータベースを使い対話型にしたり、といったことがコード数行で実現できるようになっているため、業務効率化に繋がり、ポテンシャルが非常に高いライブラリだと思います。皆さん使っていきましょう。\n",
        "\n",
        "# 9.注意事項\n",
        "1. **(再掲)社外秘はくれぐれも入力しないように**お願いします（現状、chatGPT API を使うとOpenAIにデータが飛んでしまうため）。\n",
        "2. OpenAI のAPIには無課金の場合、使用上限、使用期限があります。無料枠がなくなったら、課金しましょう、するだけの価値はありそうです。\n",
        "\n",
        "# 10.参考\n",
        "- 気になる方は[LangChainの公式ドキュメント](https://python.langchain.com/en/latest/index.html)があります。\n",
        "- 本ノートブックは、[こちらの記事](https://qiita.com/hiroki_okuhata_int/items/7102bab7d96eb2574e7d)を参考にしました。\n",
        "- プロンプトの書き方のコツについては、[こちらのサイト](https://www.promptingguide.ai/jp)が参考になります。"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mkwIxyiDLYqm"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
